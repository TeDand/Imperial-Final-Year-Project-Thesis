\chapter{Implementation} \label{chap:implementation}

This chapter presents the final implementation of the project. The full process for both the end-to-end event classification models, as well as the two-phase video reconstruction networks are given, alongside any data pre-processing that was required.

\section{End-to-end Event Classification Models}

\subsection{Data Preprocessing}

A common method of processing events streams to get integrated frames is given in \cref{ssec:frame_integration}. This technique was applied to three readily available data-sets; NMNIST, DVS128 Gesture and CIFAR10-DVS.

\begin{figure}[htb]%
    \centering
    \subfloat[\centering]{{\includegraphics[width=0.25\textwidth]{implementation/images/Dvs128_integrated_frame_1.png}}}%
    \qquad
    \subfloat[\centering]{{\includegraphics[width=0.25\textwidth]{implementation/images/Dvs128_integrated_frame_2.png}}}%
    \qquad
    \subfloat[\centering]{{\includegraphics[width=0.25\textwidth]{implementation/images/Dvs128_integrated_frame_3.png}}}%
    \caption{Three contiguous intensity frames ({a}, {b} and {c}), created from the DVS128 Gesture dataset with a person moving their right hand clockwise.}%
    \label{fig:dvs128_integrated_frames}%
\end{figure}

The integratedi frames of a hand gesture in \cref{fig:dvs128_integrated_frames} shows the motion of an arm moving in a clockwise direction. For each instance of a gesture the event stream was split into 20 frames. This made the processing of the data easy for some of the networks described later in the chapter, since the frames could be packed into 3D tensors of equal size. This means that the frames are not synchronous like they would be from an frame-based camera, and the amount of time represented by each frame is varying. As well as this, since the event streams are of varying length in the time dimension, some videos are reconstructed to a more granular scale than others. It is apparent that in this particular sample a relatively large amount of time is being compressed into every frame, resulting in a large amount of motion being visible. With more frames being created for each sample this problem would be alleviated and more easily distinguishable features may be seen. However if too many frames are taken, not only are processing times greatly increased, events may be sparse and not show any visible pattern in any one frame. A benefit of having this blurring event in the intensity frames is that the direction and degree of motion can be seen in the frames, and so the networks can also pick up these patterns in their classification process.

\color{red} TODO: Add more photos of integrated frames with more and less frames \color{black}

\color{red} TODO: Include image and explanations of frame integration \color{black}

\color{red} TODO: Add stuff about possible nlp-like networks. \color{black}

\subsubsection{Time-relative Event Segmentation}

In order to begin analysing neuromorphic data, it was pre-processed it into a form that a NN can take as input. One such method of doing so was to segment the events into groups based on their timestamp. \Cref{fig:nmnist_spikes_to_intensity_map} shows a visualisation of intensity maps created from the NMNIST\cite{NMNIST} dataset. The set of all events was split into eight segments, where each segment included events within a range of $ 1 \times 10^6 $ ms (i.e., $ 0 \rightarrow 1 $, $ 1 \rightarrow 2 $, ..., $ 7 \rightarrow 8 $). This way the data representation shifted to somewhat get back to a set of frames that mimicked the video output usually seen from everyday cameras. \textbf{(a)} shows the segmented events visualised in three dimensions (x\_location, y\_location and timestamp). In \textbf{(b)} these events were projected onto the two dimensional plane (of x\_location and y\_location), then the plots for on events and off events are shown separately. Finally in \textbf{(c)} an intensity map was created from the projected events. Each pixel in the intensity map grid was initialised to 0, and for every on event 1 was added to the cell, and for every off event 1 was subtracted from the cell. It was clear that the resulting output greatly resembled the MNIST\cite{MNIST} sample recorded by the ATIS camera (As shown in \cref{fig:nmnist_spikes_visualisation} in \cref{sec:existing_datasets}).

\begin{figure}[htb]%
    \centering
    \subfloat[\centering]{{\includegraphics[width=0.25\textwidth, height=0.7\textwidth]{implementation/images/nmnist_spikes_visualisation_segmented.png}}}%
    \qquad
    \subfloat[\centering]{{\includegraphics[width=0.4\textwidth, height=0.7\textwidth]{implementation/images/nmnist_events_segmented.png}}}%
    \qquad
    \subfloat[\centering]{{\includegraphics[width=0.25\textwidth, height=0.7\textwidth]{implementation/images/nmnist_events_heatmap_segmented.png}}}%
    \caption{A visualisation of intensity maps created by segmenting events into bins of size $ 1 \times 10^6 $ ms.}%
    \label{fig:nmnist_spikes_to_intensity_map}%
\end{figure}

Once a set of projections was made for each sample from the NMNIST dataset, it could be fed into a neural network in parallel. Ths could be done by simply flattening each intensity map and feeding all the cell values to a fully-connected dense layer in parallel, or the maps could be fed in as images to a convolutional layer. For the convolutional layer method a 3D tensor could be generated by stacking each of the intensity maps against each other and feeding them all directly into the first layer of the NN.

\section{Two-phase Intensity Reconstruction Models}

The models described above allow for the system to learn directly on the spiking data. Another approach that was taken was to attempt to utilise video reconstruction networks to the event data so that more classical models and architectures could be used to patterns in the data.

\subsection{Reconstruction Algorithms}

\subsubsection{E2VID}

E2VID, as described in \color{red} TODO: Reference background reading here \color{black}, is a state-of the art network based on UNET that reconstructs intensity videos from events data. Having gotten the output from the network (which on test data had 90\% accuracy \color{red} TODO: check accuracy figure \color{black}), it now needed to be processed slightly in order to work with the classification models. The main issue with the reconstruction was that the video were of varying lengths. In order to mitigate this additional frames were added to videos which had fewer frames than that of the longest video in the training set. These additional frames were added by repeating the video until the desired number of frames was reached.

\color{red} TODO: all of the following;

\begin{itemize}
    \item Similar to denoising network CNN more easily able to find patterns in data
    \item gesture intensity maps equivalent to 100fps but retains high frequency information without too much noise
    \item spikingjelly for integrating frames which is widely used approach
\end{itemize}

\color{black}

\section{Classification Models}

The models used to classify the either the integrated frames or video reconstructions are given below. Please note that the input shapes given below are for the NMNIST dataset, however for the other datasets different input shapes would be required (e.g. {128, 128, 2} for the DVS128 Gesture dataset).

\subsection{3D Convolutional Neural Network}

The 3D convolutional neural network in \cref{ssec:3D_conv_network} was altered to have the outputs of the hidden layer fed into a dense layer with 10 neurons to classify the correct class (0-9). Activation functions need to be present in the network to prevent all the layers from becoming equivalent to a single one \color{red} TODO: add references here \color{black} (linear regression model). In order to learn more complex patterns activation functions are a necessary aspect of creating an artificial neuron (See \cref{eq:artificial_neuron_output} in \cref{ssec:snn_and_heterogeneity}). The most commonly used activation function in deep networks (and image recognition in particular), and in this network as well, is ReLU. \color{red} TODO: add references here \color{black}. Finally, the output layer has a sigmoid activation function. This function compresses the output smoothly between the ranges of 0 and 1. this means each of the outputs from the neurons can be interpreted as a the probability of the input being any one of the 10 classes. This means we can simply take the highest probability as the predicted class from the network.

Hyper-parameter Tuning:

In order to choose the most appropriate parameters for the system, multiple tests were run varying each of the possible hyper-parameters. The tests conducted were to determine;

\begin{itemize}
    \item The number of hidden layers.
    \item The number of neurons per layer.
    \item The size of convolution kernels.
    \item The introduction of some fully connected layers after convolutional layers.
\end{itemize}

The network in each case was trained for multiple epochs. The performance of a typical network can be seen in \cref{fig:accuracy_and_loss_per_epoch}, where the performance on the training set steadily improves as the network progresses through epochs. Accuracy is one of the metrics defined in \cref{sec:evalutaion_metrics}, and the loss for the given network is called categorical cross-entropy loss. The formula used to calculate the loss is given by: $ L = -\frac{1}{N}\sum^N_{i=1}\sum^C_{c=1}y_c^{(i)}log(\hat{y}_c^{(i)}) $, where there are $ N $ samples and $ C $ classes. $ y_c^{(i)} $ is $ 1 $ when the class is correctly predicted and $ 0 $ otherwise and $ \hat{y}_c^{(i)} $ is the predicted probability of class $ c $ for data-point $ i $.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{implementation/images/accuracy_and_loss_per_epoch.png}
    \caption{A figure showing classification accuracy and cross-entropy loss per epoch on training data for a typical network.}
    \label{fig:accuracy_and_loss_per_epoch}
\end{figure}

It was clear, however, that these results may be misleading since they only represent the efficiency of the system when classifying values within the training data-set. However, when looking at the performance on an unseen test data-set, it is obvious that some of the features learnt do not easily translate to general trends in unseen data. This is known as over-fitting, and can be avoided by reducing the capacity of the data-set so that it does not learn information specific to the training set.

Model capacity is directly correlated to the n$ ^o $ of filters, as well as the number of layers, and as the model recognises more patterns in the training data, so it is important to get the optimum value for the system. The size of the kernels has an effect on the scale of the information picked up by the system. With smaller kernels more local patterns are detected, whereas with larger kernels more global effects can be seen \color{red} TODO: add image and reference here \color{black}. As for the dense layers at the end of the network, it can be seen that better results were achieved as a result of it since global patterns can be further identified after the data has been processed by the convolution layers that have picked out the most important features.

\begin{figure}[htb]%
    \centering
    \subfloat[\centering]{{\includegraphics[width=0.4\textwidth]{implementation/images/layer_tests.png}}}%
    \subfloat[\centering]{{\includegraphics[width=0.4\textwidth]{implementation/images/kernel_tests.png}}}%
    \hfill
    \subfloat[\centering]{{\includegraphics[width=0.4\textwidth]{implementation/images/layer_tests.png}}}%
    \subfloat[\centering]{{\includegraphics[width=0.4\textwidth]{implementation/images/layer_tests.png}}}%
    \caption{A visualisation of intensity maps created by segmenting events into bins of size $ 1 \times 10^6 $ ms.}%
    \label{fig:tests}%
\end{figure}

Further testing was done with increasing kernel sizes, and with pooling layers added to the network. To find local patters filter kernels smaller than the image (32x32) are used. In earlier layers the no of filters is
large and the filter size is small to capture details. The no
of filters decreases and the filter size increases later in the
network for higher-level patterns. Pooling filters the image
after convolution layers to pick out important features. \color{red} TODO: add final system architecture here \color {black}.

An overview of the layers present in the network, together with the shape of their outputs and number of trainable parameters can be seen in \cref{lst:3d_conv_layers}. It features the repeating pattern of 3D convolution layers and max-pooling. The number of filters in each layer keeps decreasing (from 512 to 256 and 128), and the size of the filters decreasing. This way when the image size decreases due to the max-pooling layers, smaller filter sizes are also used to match the scale. Since the max-pooling layers scale down the image, this also means that more large scale patterns can be identified from smaller sections of the frame.

\begin{lstlisting}[language=Bash,caption={Overview of layers in 3D convolutional network},label={lst:3d_conv_layers},numbers=none,float=htb]
_________________________________________________________________
    Layer (type)                Output Shape              Param #   
=================================================================
    conv3d_7 (Conv3D)           (None, 8, 34, 34, 512)    14336     
                                                                    
    activation_17 (Activation)  (None, 8, 34, 34, 512)    0         
                                                                    
    max_pooling3d_10 (MaxPoolin  (None, 4, 17, 17, 512)   0         
    g3D)                                                            
                                                                    
    conv3d_8 (Conv3D)           (None, 4, 17, 17, 256)    16384256  
                                                                    
    activation_18 (Activation)  (None, 4, 17, 17, 256)    0         
                                                                    
    max_pooling3d_11 (MaxPoolin  (None, 2, 8, 8, 256)     0         
    g3D)                                                            
                                                                    
    conv3d_9 (Conv3D)           (None, 2, 8, 8, 128)      11239552  
                                                                    
    activation_19 (Activation)  (None, 2, 8, 8, 128)      0         
                                                                    
    max_pooling3d_12 (MaxPoolin  (None, 1, 4, 4, 128)     0         
    g3D)                                                            
                                                                    
    flatten_3 (Flatten)         (None, 2048)              0         
                                                                    
    dense_6 (Dense)             (None, 256)               524544    
                                                                    
    activation_20 (Activation)  (None, 256)               0         
                                                                    
    dense_7 (Dense)             (None, 10)                2570      
                                                                    
    activation_21 (Activation)  (None, 10)                0         
                                                                    
=================================================================
Total params: 28,165,258
Trainable params: 28,165,258
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

\subsection{Convolutional LTSM}

An overview of the layers present in the network, together with the shape of their outputs and number of trainable parameters can be seen in \cref{lst:conv_lstm_layers}. The thought behind the structure of this network is simlar to the one when designing the 3D convolutional network. The number of filters decreases as well as the filter sizes. The difference this time is that 2D convolution is carried out on each contiguous frame of the video input before being passed through LTSM networks. The way this is implemented is with the convLTSM network described in \cref{ssec:conv_lstm}.

\begin{lstlisting}[language=Bash,caption={Overview of layers in Convolutional LTSM network.},label={lst:conv_lstm_layers},numbers=none,float=htb]
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
conv_lstm2d_6 (ConvLSTM2D)  (None, 8, 34, 34, 64)     150016    
                                                                
max_pooling3d_4 (MaxPooling  (None, 8, 17, 17, 64)    0         
3D)                                                             
                                                                
batch_normalization_6 (Batc  (None, 8, 17, 17, 64)    256       
hNormalization)                                                 
                                                                
conv_lstm2d_7 (ConvLSTM2D)  (None, 8, 17, 17, 32)     110720    
                                                                
max_pooling3d_5 (MaxPooling  (None, 8, 9, 9, 32)      0         
3D)                                                             
                                                                
batch_normalization_7 (Batc  (None, 8, 9, 9, 32)      128       
hNormalization)                                                 
                                                                
conv_lstm2d_8 (ConvLSTM2D)  (None, 9, 9, 16)          27712     
                                                                
max_pooling2d_2 (MaxPooling  (None, 5, 5, 16)         0         
2D)                                                             
                                                                
batch_normalization_8 (Batc  (None, 5, 5, 16)         64        
hNormalization)                                                 
                                                                
flatten_2 (Flatten)         (None, 400)               0         
                                                                
dense_4 (Dense)             (None, 256)               102656    
                                                                
dense_5 (Dense)             (None, 10)                2570      
                                                                
=================================================================
Total params: 394,122
Trainable params: 393,898
Non-trainable params: 224
_________________________________________________________________
\end{lstlisting}

\subsection{Custom Convolutional LSTM}

An implementation of a custom convolutional LSTM can be seen in \cref{lst:custom_conv_lstm_layers} and the implementation of the custom 2D convolutional network applied to each frame of the video can be seen in \cref{lst:custom_conv_2d_layers}. This network is the natural progression from the previous convLSTM network since it applies a more complex 2D convolution to each frame. This extra capacity is evident in the number of trainable parameters in the new network when compared to the one in \cref{lst:conv_lstm_layers}.

\begin{lstlisting}[language=Bash,caption={Overview of layers in Custom Convolutional LTSM network.},label={lst:custom_conv_lstm_layers},numbers=none,float=htb]
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
time_distributed_3 (TimeDis  (None, 8, 2048)          4887936   
tributed)                                                       
                                                                
gru_3 (GRU)                 (None, 64)                405888    
                                                                
dense_25 (Dense)            (None, 1024)              66560     
                                                                
dropout_9 (Dropout)         (None, 1024)              0         
                                                                
dense_26 (Dense)            (None, 512)               524800    
                                                                
dropout_10 (Dropout)        (None, 512)               0         
                                                                
dense_27 (Dense)            (None, 128)               65664     
                                                                
dropout_11 (Dropout)        (None, 128)               0         
                                                                
dense_28 (Dense)            (None, 64)                8256      
                                                                
dense_29 (Dense)            (None, 10)                650       
                                                                
=================================================================
Total params: 5,959,754
Trainable params: 5,959,754
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

\begin{lstlisting}[language=Bash,caption={Overview of layers in Custom 2D Convolutional network built into the Custom LSTM Network in \cref{lst:custom_conv_lstm_layers}.},label={lst:custom_conv_2d_layers},numbers=none,float=htb]
_________________________________________________________________
    Layer (type)                Output Shape              Param #   
=================================================================
    conv2d_4 (Conv2D)           (None, 8, 34, 512)        157184    
                                                                    
    activation_5 (Activation)   (None, 8, 34, 512)        0         
                                                                    
    max_pooling2d_3 (MaxPooling  (None, 4, 17, 512)       0         
    2D)                                                             
                                                                    
    conv2d_5 (Conv2D)           (None, 4, 17, 256)        3277056   
                                                                    
    activation_6 (Activation)   (None, 4, 17, 256)        0         
                                                                    
    max_pooling2d_4 (MaxPooling  (None, 2, 8, 256)        0         
    2D)                                                             
                                                                    
    conv2d_6 (Conv2D)           (None, 2, 8, 128)         1605760   
                                                                    
    activation_7 (Activation)   (None, 2, 8, 128)         0         
                                                                    
    max_pooling2d_5 (MaxPooling  (None, 1, 4, 128)        0         
    2D)                                                             
                                                                    
    flatten_1 (Flatten)         (None, 512)               0         
                                                                    
    dense_2 (Dense)             (None, 256)               131328    
                                                                    
    activation_8 (Activation)   (None, 256)               0         
                                                                    
    dense_3 (Dense)             (None, 10)                2570      
                                                                    
    activation_9 (Activation)   (None, 10)                0         
                                                                    
=================================================================
Total params: 5,173,898
Trainable params: 5,173,898
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}

\subsection{Spiking Neural Network}

\color{red} TODO: Cite nengo etc. \color{black}

\subsubsection{Synaptic Smoothing}

\subsubsection{Firing Rates}

\subsubsection{Post-training Scaling}

\subsubsection{Regularizing During Training}