\chapter{Conclusion and Further Work} \label{chap:conclusion_and_further_work}

To conclude, the testing done with the two classification pipelines shows that there are significant benefits to working within the neuromorphic paradigm as opposed to classic frame-based cameras. With the same networks, performance appears to be superior when analysing event streams using classic frame-integration, as opposed to video reconstructions for simple object recognition and classification tasks. This is perhaps due to the loss of high-frequency data and when creating the intensity reconstructions. However, this was not the case for more complex tasks such as gesture recognition. In this case the reconstructions were found to be of superior quality (perhaps due to the size and nature of the dataset used), allowing for better analysis of the recorded gestures. It is fair to assume that this is due to the larger size of the DVS128 Gesture dataset, which is a more appropriate input for the reconstruction algorithm used. Overall, the two-phase intensity reconstruction pipelines show promise, being able to harness the wealth of existing networks that exist for video classification. 

As well as this, it was found that for analysing motion in event streams, the custom frame-integration method outlined in this project achieves the best results. In gesture recognition tasks it even outperformed even the video reconstruction pipeline. This performance increase cannot be seen for object recognition, however, since temporal information is not as useful in this case. For example, on the NMNIST dataset the accuracy of this novel frame-integration method was found to be the same or worse than with classical frame-integration.

The reconstructions show that the benefits of event-based cameras can be retained while still working with intensity videos, as the reconstruction networks produce high fidelity outputs even when recording videos with lots of motion or high dynamic range of light intensities. This means that the reconstructed videos are often more appropriate for classification than if a video was taken with a frame-based camera. This is also true for integrated frames, which exploit the same benefits from event data.

Finally, testing the conversion of a 3D convolutional network to a spiking neural network showed that equivalent performance could be attained, while harnessing the benefits of sparse inter-layer communications. Close to 100\% of the performance of the non-spiking equivalent was achieved while having a spiking rate of below 1Hz. This is beneficial since the information being passed between the layers of the neural network are now sparse, meaning less communication needs to take place overall to analyse the data. This is therefore a much less power intensive, and more computationally efficient encoding affording itself to the use on low-powered chips and for real-time processing. This is especially true of the fully neuromorphic system detailed in this report, since the input is from a event-based camera, which in itself is much more power-efficient than frame-based cameras. The Legendre Memory Unit also shows promise as a good way of implementing a recurrent unit in a spiking neural network, however more work needs to be done to tune the performance.

\section{Further Work}

Having reached the end of the project, it is important to note the limitations (such as computing power and available time) that need to be acknowledged. As such, there are some key areas in which there is scope for further research and development. These include;

\begin{enumerate}
    \item Using a more capable setup to test the networks. This would also allow for the creation of more intensive networks. As well as this larger batch sizes can be used for more stable training with higher RAM available.
    \item Comparing the performance of the reconstructed pipeline like-for-like against a video stream from a frame-based camera. This has been omitted from this project due to time constraints and lack of available datasets.
    \item Changing E2VID parameters so that resulting reconstructed video is of a higher framerate.
    \item Testing the performance of the described networks and pipelines on more readily available datasets to make sure that the findings apply to other environments. Intensity reconstructions may be beneficial for more complex tasks.
    \item Looking more into the use of natural language processing techniques such as Bag of Words and Support Vector Machines when working with event streams.
    \item Creating more spiking neural networks, perhaps from scratch rather than converting regular ANNs.
    \item Tuning the LMU and its spiking equivalent to work well on the given datasets.
    \item Testing performance of spiking neural networks on intensity reconstructed video streams.
    \item Testing the performance of the end-to-end event classification networks when frame-integration is done asynchronously as well as synchronously (as described in \cref{ssec:frame_integration}). 
    \item Testing the effect of having smaller time-slices per integrated frame, so that data has an equivalent size to the E2VID output.
    \item Creating spiking neural networks based on more complex ANN networks. This involves creating approximations to other, more complex layers such as LSTM and GRU layers.
    \item Implementing spiking neural networks on neuromorphic hardware. This also opens up the possibility of real-time inference.
    \item Creating a spiking version of the intensity reconstruction algorithms to check if they still perform well.
\end{enumerate}

\section{Similar External Work}

While work was undertaken for this project, similar works have been published by external researchers. It is important to acknowledge their contributions and any similarities that have arisen as a result of working in parallel on a similar problem.

A method, similar to the custom frame integration proposed in \cref{sssec:custom_frame_integration_implementation}, that aimed to preserve temporal resolution was proposed by Lo\"ic Cordone \textit{et al.}, who proposed `voxel cubes'\cite{MiniVovelCubes}. For this method there were still binary events in every channel, but there were more than two channels so that the events in each time-slice could be subdivided into each channel. When compared to this method, the temporal information can be stored in the same way with the novel method proposed in this project, while keeping data size small since it is just a one-channel image. 

More work by Lo\"ic Cordone \textit{et al.}\cite{OtherSnnWork}, shows testing with sparse SNN architectures (using surrogate training as done in this report). These achieved similar performance on the DVS dataset to the networks outlined in this report. In their paper a 2D spiking convolutional neural network is compared to a traditional 3D convolutional network, and the training and inference times are outlined to show the efficiency of spiking encoding.
