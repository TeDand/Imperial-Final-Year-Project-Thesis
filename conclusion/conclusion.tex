\chapter{Conclusion and Further Work} \label{chap:conclusion_and_further_work}

To conclude, the testing done with the two classification pipelines shows that there are significant benefits to working within the neuromorphic paradigm as opposed to classic frame-based cameras. With the same networks, performance appears to be superior when analysing event streams using classical frame-integration, as opposed to video frames for simple object recognition and classification tasks. Secondly, the two-phase intensity reconstruction pipelines show promise, being able to harness the wealth of existing networks that exist for video classification. 

The performance with the same classification networks directly on the events rather than the video reconstructions tended to have better performance for simple object recognition and classification tasks. This is perhaps due to the loss of high-frequency data and when creating the intensity maps. However, this was not the case for more complex tasks such as gesture recognition. In this case the reconstructions were found to be of superior quality (perhaps due to the size and nature of the dataset used), allowing for better analysis of the recorded gestures. It is fair to assume that this is due to the larger size of the DVS128 Gesture dataset, which is a more appropriate input for the reconstruction algorithm used.

As well as this, it was found that for analysing motion in event streams, the custom frame-integration method outlined in this project achieves the best results. In gesture recognition tasks it even outperformed the video reconstruction pipeline. This performance increase cannot be seen for object recognition, however, since temporal information is not as useful in this case. For example, on the NMNIST dataset the accuracy of this novel frame-integration method was found to be the same or worse than with classical frame-integration.

The reconstructions show that the benefits of event-based cameras can be retained even when working with intensity videos, as the reconstruction networks produce high fidelity outputs even when recording videos with lots of motion or high dynamic range of light intensities. This means that the reconstructed videos are often more appropriate for classification that if a video was taken with a frame-based camera. This is also true for integrated frames, which exploit the same benefits from event data.

Finally, testing the conversion of a 3D convolutional network showed that equivalent performance could be attained, while harnessing the benefits of spiking neural networks. Close to 100\% of the performance of the non-spiking equivalent was achieved while having a spiking rate of below 1Hz. This is beneficial since the information being passed between the layers of the neural network are now sparse, meaning less communication needs to take place overall to analyse the data. This is therefore a much less power intensive, and more computationally efficient encoding affording itself to the use on low-powered chips and for real-time processing. This is especially true of the fully neuromorphic system detailed in this report, since the input is from a event-based camera, which in itself is much more power-efficient than frame-based cameras. The Legendre Memory Unit also shows promise as a good way of implemented a recurrent unit in a spiking neural network, however more work needs to be done to tune the performance.

\section{Further Work}

Having reached the end of the project, it is important to note the limitations (such as computing power and available time) that need to be acknowledged. As such, there are some key areas in which there is scope for further research and development. These include;

\begin{enumerate}
    \item With a more capable setup and time to test the networks, there is the opportunity to create more intensive networks. As well as this larger batch sizes can be used for more stable training with higher RAM available.
    \item Compare the performance of the reconstructed pipeline like-for-like against a video stream from a frame-based camera. This has been omitted from this project due to time constraints and lack of available datasets.
    \item Change E2VID parameters so that resulting reconstructed video is of a higher framerate.
    \item Test the performance of the described networks and pipelines on more readily available datasets to make sure that the findings apply to other environments. Intensity reconstructions may be beneficial for more complex tasks.
    \item Look more at the use of natural language processing techniques such as Bag of Words and Support Vector Machines when working with event streams.
    \item Create more spiking neural networks, perhaps from scratch rather than converting regular ANNs. Further testing needs to be done in this area.
    \item Tune the LMU to work well on the given datasets.
    \item Test performance of spiking neural networks on intensity reconstructed video streams.
    \item Test the performance of the end-to-end event classification networks when frame-integration is done asynchronously as well as synchronously (as described in \cref{ssec:frame_integration}). 
    \item The effect of having smaller time-slices per integrated frame need to be tested, where the data has an equivalent size to the E2VID output.
    \item Create spiking neural networks based on more complex ANN networks. This involves creating approximations to other, more complex layers such as LSTM and GRU layers.
    \item Implement spiking neural networks on neuromorphic hardware. This also opens up the possibility of real-time inference.
    \item Create a spiking version of the intensity reconstruction algorithms to check if they perform well.
\end{enumerate}

\section{Similar External Work}

A method, similar to the custom frame integration proposed in \cref{sssec:custom_frame_integration_implementation}, that aimed to preserve temporal resolution was proposed by Lo\"ic Cordone \textit{et al.}, who proposed `voxel cubes'\cite{MiniVovelCubes}. For this method there were still binary events in every channel, but there were more than two channels so that the events in each time-slice could be subdivided into each channel. When compared to this method, the temporal information can be stored in the same way with the novel method proposed in this project, while keeping data size small since it is just a one-channel image. 

More work by Lo\"ic Cordone \textit{et al.}\cite{OtherSnnWork}, shows testing with sparse SNN architectures (using surrogate training as done in this report) achieving similar performance on the DVS dataset to the networks outlined in this report. In their paper a 2D spiking convolutional neural network is compared to a 3D convolutional network, and the training and inference times are outlines to show the efficiency of spiking encoding.
