\chapter{Evaluation} \label{chap:evaluation}

This chapter aims to follow on from the previous one which showed the results of each individual network. It serves as a high level overview of the performances of the end-to-end event classification pipeline and two-phase video reconstruction pipeline. There are also comparisons of each of the pipelines, allowing for the discerning their benefits and shortcomings.

\section{Comparison of Classification Pipelines}

When comparing the two-phase video reconstruction pipeline to the direct event classification one, the results show that in general the direct event analysis leads to superior classification accuracies. The reason for this could be the loss of some high frequency data when creating intensity reconstructions with event data. The reason for this is that the created video is of a certain frame-rate, which is much lower than the temporal resolution of events captured by event cameras (which are on the $ \mu s $ level scale). There is, however, scope to still use the reconstruction pipeline without losing too much temporal information. For this the number of events per input to the E2VID network can be reduced so that the resulting video is of a higher frame-rate. However, this would require for the network to be retrained to achieve good results on the data with fewer data-points per frame.

Moreover, in the case of NMNISt, the reconstructions were not too dissimilar to the integrated frames. In both pipelines the input was similar to an edge map, and so the performance difference was less noticeable. This may be because of the small size of the dataset frames, and so for better reconstruction performance the E2VID network would have to be re-trained. Overall the performance of the two-phase pipeline was lower than working directly on the integrated frames since even though both inputs were like edge maps, the integrated frames retained far more temporal data.

For the DVS128 Gesture dataset, the performance difference between the two pipelines was far smaller. In this case the reconstructions were much more realistic and well-defined, allowing for the classification networks to more easily find patterns in the videos. However, the integrated frames still retained more information about the motion of the person since events for movement were visible in each frame, and so there was more information for each network to work with when finding patterns for each gesture.

Overall, for more complex tasks such as gesture recognition, video reconstruction had better performance than with more simple classification problems. It is fair to assume this trend extrapolates to more complex classification tasks, since the intensity reconstructions add more detail to frames than are available in the events they process. As well as this, when tested on the Event Camera dataset, the reconstructions were much clearer visually than the respective video frames. As well as this the more complex details from the environment were also reconstructed, meaning the there is more information held in the reconstructed frames than in the events they were generated from. This could be incredibly beneficial for more complex and fine-grained classification tasks.

\section{Evaluation of Classification Networks}

The benefits of each classification network were clear when looking at their performance on each dataset. The 3D convolutional network had the most trainable parameters (and as such the highest training and inference times), which lended itself to better performance on the NMNIST dataset. The dataset's scale is small enough for the network to learn all possible features and get a good classification accuracy. For the more complex cases of gesture recognition, however, the performance of this network lagged behind the reccurrent LSTM networks (especially the custom one). The reason for this is that in these cases temporal patterns are much more important, and LSTMs are ideal to learn such features. The 3D convolutional network could no longer memorise enough features to accurately classify all gestures, and even often over-fit leading to lower accuracies on the unseen testing data.

\color{red} TODO: Write about spiking neural networks. \color{black}

% \color{red} TODO: Write out the following;

% \begin{itemize}
    % \item Loss of high frequency data
    % \item conventional networks built around frame based vision
    % \item for NMNIST the small size of images means that reconstructions do no vary much from the rudimentary intensity maps (could be used as an illustration of how reconstruction works).
    % \item gestures are more complicated to characterise, since the networks for NMIST can easily learn all possible features. As well as this the image size for NMNIST is very small
    % \item intensity maps can be of much higher framerate since events are taken asynchronously
    % \item gesture intensity maps equivalent to 100fps but retains high frequency information without too much noise
    % \item most gesture recognition datasets involve removing backgrounds and focussing on the hand, our network automatically only sees moving objects
    % \item spiking neural networks better able to exploit features from spiking data than reconstructed data
% \end{itemize}

% \color{black}